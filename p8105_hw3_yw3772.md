P8105 HW3
================
Yushan Wang

``` r
library(tidyverse)
```

    ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──

    ## ✓ ggplot2 3.3.5     ✓ purrr   0.3.4
    ## ✓ tibble  3.1.4     ✓ dplyr   1.0.7
    ## ✓ tidyr   1.1.3     ✓ stringr 1.4.0
    ## ✓ readr   2.0.1     ✓ forcats 0.5.1

    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## x dplyr::filter() masks stats::filter()
    ## x dplyr::lag()    masks stats::lag()

``` r
library(p8105.datasets)
```

# Problem 1

``` r
data("instacart")
```

The “instacart” dataset has 15 columns(variables) and 1384617 rows.

The meaning of some of the key variables are listed below: (source:
<https://www.p8105.com/dataset_instacart.html>)

`add_to_cart_order: order in which each product was added to cart`

`reordered: 1 if this prodcut has been ordered by this user in the past, 0 otherwise`

`eval_set: which evaluation set this order belongs in (Note that the data for use in this class is exclusively from the “train” eval_set)`

`order_number: the order sequence number for this user (1=first, n=nth)`

`order_dow: the day of the week on which the order was placed`

`order_hour_of_day: the hour of the day on which the order was placed`

`days_since_prior_order: days since the last order, capped at 30, NA if order_number=1`

The variables `eval_set`, `product_name`, `aisle`, and `department` are
all characters. all other variables are intergers.

The distribution summary of the variables of
`add_to_cart_order`,`order_number`, `order_dow`, `order_hour_of_day`,
`days_since_prior_order` are shown below:

``` r
summary(instacart[c(3,7,8,9,10)])
```

    ##  add_to_cart_order  order_number      order_dow     order_hour_of_day
    ##  Min.   : 1.000    Min.   :  4.00   Min.   :0.000   Min.   : 0.00    
    ##  1st Qu.: 3.000    1st Qu.:  6.00   1st Qu.:1.000   1st Qu.:10.00    
    ##  Median : 7.000    Median : 11.00   Median :3.000   Median :14.00    
    ##  Mean   : 8.758    Mean   : 17.09   Mean   :2.701   Mean   :13.58    
    ##  3rd Qu.:12.000    3rd Qu.: 21.00   3rd Qu.:5.000   3rd Qu.:17.00    
    ##  Max.   :80.000    Max.   :100.00   Max.   :6.000   Max.   :23.00    
    ##  days_since_prior_order
    ##  Min.   : 0.00         
    ##  1st Qu.: 7.00         
    ##  Median :15.00         
    ##  Mean   :17.07         
    ##  3rd Qu.:30.00         
    ##  Max.   :30.00

**Count asiles and find most ordered aisles**

``` r
instacart %>%
  count(aisle, name = "n_obs") %>% 
  arrange(desc(n_obs)) 
```

    ## # A tibble: 134 × 2
    ##    aisle                          n_obs
    ##    <chr>                          <int>
    ##  1 fresh vegetables              150609
    ##  2 fresh fruits                  150473
    ##  3 packaged vegetables fruits     78493
    ##  4 yogurt                         55240
    ##  5 packaged cheese                41699
    ##  6 water seltzer sparkling water  36617
    ##  7 milk                           32644
    ##  8 chips pretzels                 31269
    ##  9 soy lactosefree                26240
    ## 10 bread                          23635
    ## # … with 124 more rows

As shown in the counting table above, there are 134 aisles. Fresh
vegetables is the aisles that most items ordered from.

**Plot of number of items ordered in each aisle**

``` r
instacart %>%
  count(aisle, name = "n_obs") %>% 
  filter(n_obs > 10000) %>% 
  ggplot(aes(x = aisle, y = n_obs))+
  geom_point() +
  coord_flip() +
  ggtitle("") +
  xlab("aisle name") + ylab("number of items")
```

![](p8105_hw3_yw3772_files/figure-gfm/unnamed-chunk-5-1.png)<!-- -->

**Table showing the three most popular items in each of the aisles
“baking ingredients”, “dog food care”, and “packaged vegetables
fruits”**

``` r
baking_ingredients = 
  instacart %>%
  filter(aisle == "baking ingredients") %>% 
  group_by(aisle, product_name) %>%
  summarize(n_obs = n()) %>% 
  arrange(desc(n_obs)) 
```

    ## `summarise()` has grouped output by 'aisle'. You can override using the `.groups` argument.

``` r
dog_food_care = 
  instacart %>%
  filter(aisle == "dog food care") %>% 
  group_by(aisle, product_name) %>%
  summarize(n_obs = n()) %>% 
  arrange(desc(n_obs)) 
```

    ## `summarise()` has grouped output by 'aisle'. You can override using the `.groups` argument.

``` r
packaged_vegetables_fruits = 
  instacart %>%
  filter(aisle == "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>%
  summarize(n_obs = n()) %>% 
  arrange(desc(n_obs)) 
```

    ## `summarise()` has grouped output by 'aisle'. You can override using the `.groups` argument.

``` r
three_most_popular = 
  bind_rows(baking_ingredients[c(1,2,3),],dog_food_care[c(1,2,3),],packaged_vegetables_fruits[c(1,2,3),])

three_most_popular
```

    ## # A tibble: 9 × 3
    ## # Groups:   aisle [3]
    ##   aisle                      product_name                                  n_obs
    ##   <chr>                      <chr>                                         <int>
    ## 1 baking ingredients         Light Brown Sugar                               499
    ## 2 baking ingredients         Pure Baking Soda                                387
    ## 3 baking ingredients         Cane Sugar                                      336
    ## 4 dog food care              Snack Sticks Chicken & Rice Recipe Dog Treats    30
    ## 5 dog food care              Organix Chicken & Brown Rice Recipe              28
    ## 6 dog food care              Small Dog Biscuits                               26
    ## 7 packaged vegetables fruits Organic Baby Spinach                           9784
    ## 8 packaged vegetables fruits Organic Raspberries                            5546
    ## 9 packaged vegetables fruits Organic Blueberries                            4966

**Table showing the mean hour of the day at which Pink Lady Apples and
Coffee Ice Cream are ordered**

Table showing the mean hour of the day at which Pink Lady Apples are
ordered

``` r
Pink_Lady_Apples = 
  instacart %>% 
  filter(product_name == "Pink Lady Apples") %>% 
  group_by(order_dow) %>% 
  summarize(mean_hour_of_the_day = mean(order_hour_of_day))
```

Table showing the mean hour of the day at which Coffee Ice Cream are
ordered

``` r
Coffee_Ice_Cream = 
  instacart %>% 
  filter(product_name == "Coffee Ice Cream") %>% 
  group_by(order_dow) %>% 
  summarize(mean_hour_of_the_day = mean(order_hour_of_day))
```

Merge two tables

``` r
meanhour_table = 
  merge(x = Pink_Lady_Apples, y = Coffee_Ice_Cream, by = "order_dow") %>%  
  rename(Pink_Lady_Apples_mean_hour = mean_hour_of_the_day.x, 
         Coffee_Ice_Cream_mean_hour = mean_hour_of_the_day.y)
```

Take mean of apple & ice cream mean hour

``` r
meanhour_table %>% 
  mutate(mean_hour = ((Pink_Lady_Apples_mean_hour+Coffee_Ice_Cream_mean_hour)/2) ) %>% 
  select(order_dow, mean_hour)
```

    ##   order_dow mean_hour
    ## 1         0  13.60769
    ## 2         1  12.83789
    ## 3         2  13.54154
    ## 4         3  14.78409
    ## 5         4  13.38456
    ## 6         5  12.52374
    ## 7         6  12.88542

## Problem 2

``` r
data("brfss_smart2010") 
```

Clean the data

``` r
brfss_smart2010 = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  filter(response == c("Excellent", "Very good", "Good", "Fair", "Poor")) %>% 
  mutate(response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent")))
```

Answer some questions: **In 2002, which states were observed at 7 or
more locations? What about in 2010?**

``` r
brfss_smart2010 %>% 
  filter(year == "2002") %>% 
  group_by(locationabbr) %>% 
  summarize(n_obs = n_distinct(locationdesc)) %>% 
  filter(n_obs >= 7)
```

    ## # A tibble: 0 × 2
    ## # … with 2 variables: locationabbr <chr>, n_obs <int>

There were zero states observed at 7 or more locations in 2002.

``` r
brfss_smart2010 %>% 
  filter(year == "2010") %>% 
  group_by(locationabbr) %>% 
  summarize(n_obs = n_distinct(locationdesc)) %>% 
  filter(n_obs >= 7)
```

    ## # A tibble: 9 × 2
    ##   locationabbr n_obs
    ##   <chr>        <int>
    ## 1 CA              11
    ## 2 CO               7
    ## 3 FL              30
    ## 4 MA               8
    ## 5 MD               9
    ## 6 NE               9
    ## 7 NJ              16
    ## 8 NY               7
    ## 9 TX              13

There were 9 states observed at 7 or more locations in 2010. They are
CA, CO, FL, MA, MD, NE, NJ, NY, TX.

**Construct a new dataset that is limited to Excellent responses, and
contains, year, state, and a variable that averages the data\_value**

``` r
brfss_smart2010_new = 
  brfss_smart2010 %>% 
  filter(response == "Excellent") %>% 
  group_by(year, locationabbr) %>% 
  summarize(mean_data_value = mean(data_value,na.rm = TRUE))
```

    ## `summarise()` has grouped output by 'year'. You can override using the `.groups` argument.

**Make a “spaghetti” plot**

``` r
brfss_smart2010_new %>% 
  ggplot(aes(x = locationabbr , y = mean_data_value)) +
  geom_line() +
  coord_flip()
```

![](p8105_hw3_yw3772_files/figure-gfm/unnamed-chunk-16-1.png)<!-- -->
**Make a two-panel plot** Make a two-panel plot showing, for the years
2006, and 2010, distribution of data\_value for responses (“Poor” to
“Excellent”) among locations in NY State.

``` r
brfss_smart2010 %>% 
  filter(year == c(2006, 2010)) %>% 
  filter(locationabbr == "NY") %>% 
  ggplot(aes(x = response, y = data_value))+
  geom_point() + 
  facet_grid(.~year)
```

    ## Warning in year == c(2006, 2010): longer object length is not a multiple of
    ## shorter object length

![](p8105_hw3_yw3772_files/figure-gfm/unnamed-chunk-17-1.png)<!-- -->

## Problem 3

Load and tidy the data

``` r
accel_df =
  read_csv("data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(day = factor(day))
```

    ## Rows: 35 Columns: 1443

    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr    (1): day
    ## dbl (1442): week, day_id, activity.1, activity.2, activity.3, activity.4, ac...

    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

There are 35 observations and 1443 variables in this dataset. The
variables are `week`, `day_id`, `day`, and 1440 activity variables like
`activity_1`, `activity_2`… to `activity_1440`.

**Traditional analyses of accelerometer data**

``` r
total_activity = 
  accel_df %>% 
  mutate(total_activity = sum(accel_df[4:1443])) %>% 
  subset(select = c("week","day_id","day","total_activity" ))

total_activity
```

    ## # A tibble: 35 × 4
    ##     week day_id day       total_activity
    ##    <dbl>  <dbl> <fct>              <dbl>
    ##  1     1      1 Friday         13459021.
    ##  2     1      2 Monday         13459021.
    ##  3     1      3 Saturday       13459021.
    ##  4     1      4 Sunday         13459021.
    ##  5     1      5 Thursday       13459021.
    ##  6     1      6 Tuesday        13459021.
    ##  7     1      7 Wednesday      13459021.
    ##  8     2      8 Friday         13459021.
    ##  9     2      9 Monday         13459021.
    ## 10     2     10 Saturday       13459021.
    ## # … with 25 more rows

The total activity in each day through five weeks are all the same.

**Make plot**

``` r
accel_df %>%
  pivot_longer(
    activity_1:activity_1440, 
    names_to = "activity", 
    values_to = "activity_count_per_minute") %>%
   mutate(
    activity = factor(activity)
  ) %>% 
  ggplot(aes(x = day_id, y = activity_count_per_minute, colour = day_id)) +
  geom_point()
```

![](p8105_hw3_yw3772_files/figure-gfm/unnamed-chunk-20-1.png)<!-- -->

The graph shows that this person mainly had their activity count per
minute below 2500.
